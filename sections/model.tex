\section{Our method}

The goal of our model is to identify "single hit" on modular detectors in
real-time data collection.  During training, our model takes a set of image
triplets and their corresponding label triplets as inputs.  Each element in an
image triplet is a quadrant of area from the high-speed pnCCD detector primarily
used in the AMO hutch at LCLS, given that there is no time to assemble panels
into a whole SPI image.  We present two components in our SPI classification
model: (1) An image encoder with a convolutional neural network (CNN)
architecture that converts input images into embeddings in a latent vector space;
(2) Siamese neural networks that are employed for training the CNN image encoder
to discriminate between differently labeled images.  We then describe the
training strategies and model inferences.  


\subsection{Image encoder, siamese neural networks and triplet loss}

Our image encoder consists of two convolutional layers and a fully connected
layers.  The first convolutional layer uses a $5 \times 5$ single channel filter
with a stride of one and no padding.  The second convolutional layer employs a
32-channel $5 \times 5$ filter with a with a stride of one and no padding.  A
ReLU activation function is applied to the outcome of each convolutional layer,
which is followed by a max-pooling operation performed by a $2\times 2$ filter
with a stride of two.  Then, a fully connected layer is used to generate the
final image embedding with the size of 128.  As a result, all input images are
encoded into embeddings in a latent space, where embedding distance between two
images, estimated with $L^2$ squared distance, is a measurement of their
similarities.  Namely, images of the same label should have a small embedding
distance, whereas images of different labels should have a large embedding
distance.  

%% row x col x filters
%% \renewcommand{\arraystretch}{1.2}
%% pnCCD: 
\begin{table}
\resizebox{1.0\textwidth}{!}{
    \begin{tabular}{  | l | l l l l l | }
        \hline
        Layer       &  In channels    & Out channels    & Kernel size & Stride & Padding \\
        \hline
        Conv_1      &  1              & 32              & 5           & 1      & 0 \\
        ReLU_1      &  -              & -               & -           & -      & - \\
        MaxPool2D_1 &  -              & -               & 2           & 2      & 0 \\
        Dropout_1   &  -              & -               & -           & -      & - \\
        \hline
        Conv_2      &  32             & 64              & 5           & 1      & 0 \\
        ReLU_2      &  -              & -               & -           & -      & - \\
        MaxPool2D_2 &  -              & -               & 2           & 2      & 0 \\
        Dropout_2   &  -              & -               & -           & -      & - \\
        \hline
        Layer       &  In features & Out features       & - & - & - \\
        \hline
        FC_3        &  *              & 512             & -           & -      & - \\
        ReLU_3      &  -              & -               & -           & -      & - \\
        \hline
        FC_4        &  512            & 128             & -           & -      & - \\
        \hline
    \end{tabular}
}
\caption{Our neural network architecture of the image encoder.  {\color{red}
This whole table can be replaced by an illustrative figure.}}
\label{table:nn_arch}
\end{table}

How to train the encoder to understand embedding similarities?  Siamese neural
networks propose to pass a triplet of images into the same image encoder, which
gives rise to a triplet of embeddings.  Each image/embedding triplet comprises
an anchor as a reference, a positive with a label identical to the anchor, and a
negative with a label different from the anchor.  Concretely, an
input image $x$ is embedded into a $d$-dimensional hypersphere with the radius
of one in a latent space through a CNN encoder $f$, or $f(x) \in \mathbb{R}^d$
and $\|f(x)\|_2=1$.  Given $N$ triplets of anchor $x^a$, positive $x^p$ and
negative $x^n$, the CNN encoder $f$ should learn to meet the following training
criterion,

\newcommand{\isep}{\mathrel{{.}\,{.}}\nobreak}
\begin{equation} \label{eq:1}
    \begin{array}{l}
       \|f(x_i^a) - f(x_i^p)\|_2^2 + \alpha < \|f(x_i^a) - f(x_i^n)\|_2^2, \, i = 1 \isep N
    \end{array}
\end{equation}

where the squared embedding distance between $i$-th pair of positive and anchor
is enforced to be shorter than the squared embedding distance between $i$-th
pair of negative and anchor by a margin of $\alpha$.  A triplet loss function,
shown below, is thus employed to incentivize the CNN encoder $f$ to satisfy the
training criterion in Eq. (\ref{eq:1}).

\begin{equation}
    \sum_{i=1}^{N} \left[ \alpha + \|f(x_i^a) - f(x_i^p)\|_2^2 - \|f(x_i^a) -
    f(x_i^n)\|_2^2 \right]_+
\end{equation}


\subsection{Trick in training: choose semi-hard triplets}

One straightforward solution to forming a triplet can be done in two steps: (1)
Randomly choose one class, from which drawing two examples as anchor and
positive, respectively; (2) Randomly choose one example from classes different
from the previously chosen class.  This solution might not deliver a fast
convergence when there are many easy triplets, namely, embeddings of the same
class are already closer to embeddings of different classes.  One trick to
reduce the abundance of easy triplets is to deliberately select hard triplets
that violate the training criterion in Eq. (\ref{eq:1}).  This trick should be
done at the mini batch level but not throughout all input images, as some badly
imaged inputs or edge cases would appear repeatedly in triplets.  More
specifically, we choose seimi-hard triplets during training, meaning that, given
an anchor, the squared distance from the negative embedding should be larger
than the squared distance from the positive embedding but not larger by a margin
of $\alpha$, as depicted in Eq. (\ref{eq:3}).

\begin{equation}\label{eq:3}
    \|f(x_i^a) - f(x_i^p)\|_2^2 \;<\; \|f(x_i^a) - f(x_i^n)\|_2^2 \;<\; \|f(x_i^a) - f(x_i^p)\|_2^2 + \alpha, \, i = 1 \isep N
\end{equation}


\subsection{Optimization}

%% Adam optimization algorithm

We trained our CNN using Adam \cite{kingmaAdamMethodStochastic2017} with the
learning rate of $10^{-3}$.  The CNN weights are initialized to random values
from a Gaussian probability distribution with a mean of $0.0$ and a standard
deviation of $0.2$.  

\subsection{Data preprocessing}

We use a quadrant of area, called a panel, of a pnCCD detector for training, as
shown in the Fig. (XXX).  Thanks to the alignment of X-ray beam center with
detector center before data collection, it's less likely for a panel to
consistently recieve substantially more photons than other panels.  Therefore,
we decide to vertically or horizontally flip some panels to ensure X-ray
scattering patterns appear at the bottom right territory.  Morever, cropping the
area that has scattering patterns also enhances model training, as our model
doesn't have to learn from areas with little salient features, e.g. areas of
noise or systematic errors.  Then, bad pixel areas of the pnCCD detector are
masked out with zero values before training.  Since those areas deteriorate over
time, if not masked, they would have steered our model to learn nuanced
differences not intrinsic in scattering patterns, eventually undermining the
model accuracy. Lastly, to improve model accuracy, each panel is normalized by
Z-score normalization, namely allowing the normalized values to have the mean of
zero and the standard deviation of one.  

