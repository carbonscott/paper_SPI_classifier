




















\section{Our model and training strategy for real-time hit classification}

The goal of our model is to identify \textit{single-hit} while a SPI diffraction
data collection is underway.  From a model perspective, we essentially use the
same neural network architecture to create an experiment-specific hit classifier
and a generalized hit classifier.  We focus more on explaining how our
experiment-specific classifier works, considering it will have an immediate
impact on real-time classification.  We illustrate the training strategy of a
generalized model afterwards.  


\subsection{CNN vision backbone}

Our CNN vision backbone consists of two convolutional layers that extract image
features and two fully connected layers that compress image features into low
dimensional embeddings.  The first convolutional layer uses a $5 \times 5$
single channel filter with a stride of one and no padding.  The second
convolutional layer employs a 32-channel $5 \times 5$ filter with a with a
stride of one and no padding.  A ReLU activation function is applied to the
outcome of each convolutional layer, which is followed by a max-pooling
operation performed by a $2\times 2$ filter with a stride of two.  Then, two
fully connected layer are used to generate the final embedding with the size of
128.  As a result, all SPI diffraction images are encoded into embeddings in a
latent space, where embedding distance between two images, estimated with $L^2$
squared distance, is a measurement of their similarities.  Expectedly,
identically labelled hits should have a small embedding distance, whereas
differently labelled hits should have a large embedding distance.  

% Maybe discuss how many parameters are involved depending on the image cropping
% and downsizing in data preprocess.  

%% row x col x filters
%% \renewcommand{\arraystretch}{1.4}
%% pnCCD: 
\begin{table}
\centering
\resizebox{1.0\textwidth}{!}{
    \begin{tabular}{   l | l l l l l  }
        \hline
        Layer       &  In channels    & Out channels    & Kernel size & Stride       & Padding     \\
        \hline
        Conv 1      &  1              & 32              & 5           & 1            & 0           \\
        ReLU 1      &  \textemdash    & \textemdash     & \textemdash & \textemdash  & \textemdash \\
        MaxPool2D 1 &  \textemdash    & \textemdash     & 2           & 2            & 0           \\
        Dropout 1   &  \textemdash    & \textemdash     & \textemdash & \textemdash  & \textemdash \\
        \hline
        Conv 2      &  32             & 64              & 5           & 1            & 0           \\
        ReLU 2      &  \textemdash    & \textemdash     & \textemdash & \textemdash  & \textemdash \\
        MaxPool2D 2 &  \textemdash    & \textemdash     & 2           & 2            & 0           \\
        Dropout 2   &  \textemdash    & \textemdash     & \textemdash & \textemdash  & \textemdash \\
        \hline
        Layer       &  In features    & Out features    & \textemdash & \textemdash  & \textemdash \\
        \hline
        FC 3        &  *              & 512             & \textemdash & \textemdash  & \textemdash \\
        ReLU 3      &  \textemdash    & \textemdash     & \textemdash & \textemdash  & \textemdash \\
        \hline
        FC 4        &  512            & 128             & \textemdash & \textemdash  & \textemdash \\
        \hline
    \end{tabular}
}
\caption{Our neural network architecture of the image encoder.
{\color{red} This whole table can be replaced by an illustrative figure.}}
\end{table}


\subsection{Triplet loss function for training}

How to train the CNN vision backbone to distinguish SPI hits?  We use a strategy
adopted from FaceNet \cite{schroffFaceNetUnifiedEmbedding2015} that uses a
triplet loss function for training the vision backbone.  Each input consists of
a triplet of diffraction images, which are called an anchor $x^a$, a positive
$x^p$ and a negative $x^n$. The anchor shares the same label as the positive but
different from the negative.  Two pairs will be put into a comparison in the
model. The postive pair contains the anchor and the positive $(x^a, x^p)$,
whereas the negative pair include the anchor and the negative $(x^a, x^n)$.
During training, three identical CNNs $f$ extract features from a triplet $(x^a,
x^p, x^n)$, while another component in the model measures the squared $L_2$
distance in the positive pair $\|f(x^a) - f(x^p)\|_2^2$ and the negative pair
$\|f(x^a) - f(x^n) \|_2^2$, respectively.  The objective of training our CNN $f$
is to separate the negative pair $(f(x^a), f(x^n))$ more than the the positive
pair $(f(x^a), f(x^p))$ by at least a margin $\alpha$.  More specifically, we
enforce that every embedding has a unit length of one in a $d$ dimensional
latent space, namely $f(x) \in \mathbb{R}^d$ and $\|f(x) \|_2=1$.  It means that
a SPI diffraction image will be mapped to a single point on a $d$-dimensional
hypersphere with a radius of one.  

A pictorial description
of 


Siamese neural
networks propose to pass a triplet of images into the same image encoder, which
gives rise to a triplet of embeddings.  Each image/embedding triplet comprises
an anchor as a reference, a positive with a label identical to the anchor, and a
negative with a label different from the anchor.  Concretely, an input image $x$
is embedded into a $d$-dimensional hypersphere with the radius of one in a
latent space through a CNN encoder $f$, or $f(x) \in \mathbb{R}^d$ and $\|f(x)
\|_2=1$.  Given $N$ triplets of anchor $x^a$, positive $x^p$ and negative $x^n$,
the CNN encoder $f$ should learn to meet the following training criterion,

\newcommand{\isep}{\mathrel{{.}\,{.}}\nobreak}
\begin{equation} \label{eq:1}
    \begin{array}{l}
       \|f(x_i^a) - f(x_i^p)\|_2^2 + \alpha < \|f(x_i^a) - f(x_i^n)\|_2^2, \, i = 1 \isep N
    \end{array}
\end{equation}

where the squared embedding distance between $i$-th pair of positive and anchor
is enforced to be shorter than the squared embedding distance between $i$-th
pair of negative and anchor by a margin of $\alpha$.  A triplet loss function,
shown below, is thus employed to incentivize the CNN encoder $f$ to satisfy the
training criterion in Eq. (\ref{eq:1}).

\begin{equation}
    \sum_{i=1}^{N} \left[ \alpha + \|f(x_i^a) - f(x_i^p)\|_2^2 - \|f(x_i^a) -
    f(x_i^n)\|_2^2 \right]_+
\end{equation}


\subsection{Selection of semi-hard triplets}

One straightforward solution to forming a triplet can be done in two steps: (1)
Randomly choose one class, from which drawing two examples as anchor and
positive, respectively; (2) Randomly choose one example from classes different
from the previously chosen class.  This solution might not deliver a fast
convergence when there are many easy triplets, namely, embeddings of the same
class are already closer to embeddings of different classes.  One trick to
reduce the abundance of easy triplets is to deliberately select hard triplets
that violate the training criterion in Eq. (\ref{eq:1}).  This trick should be
done at the mini batch level but not throughout all input images, as some badly
imaged inputs or edge cases would appear repeatedly in triplets.  More
specifically, we choose seimi-hard triplets during training, meaning that, given
an anchor, the squared distance from the negative embedding should be larger
than the squared distance from the positive embedding but not larger by a margin
of $\alpha$, as depicted in Eq. (\ref{eq:3}).

\begin{equation}\label{eq:3}
    \begin{aligned}[b]
    &\|f(x_i^a) - f(x_i^p)\|_2^2 \;<\; \|f(x_i^a) - f(x_i^n)\|_2^2 \;<\;
    \|f(x_i^a) - f(x_i^p)\|_2^2 + \alpha, \\
    &i = 1 \isep N
    \end{aligned}
\end{equation}


\subsection{Optimization}

%% Adam optimization algorithm

We trained our CNN using Adam \cite{kingmaAdamMethodStochastic2017} with the
learning rate of $10^{-3}$.  The CNN weights are initialized to random values
from a Gaussian probability distribution with a mean of $0.0$ and a standard
deviation of $0.2$.  

\subsection{Data preprocessing}

We use a quadrant of area, called a panel, of a pnCCD detector for training, as
shown in the Fig. (XXX).  Thanks to the alignment of X-ray beam center with
detector center before data collection, it's less likely for a panel to
consistently recieve substantially more photons than other panels.  Therefore,
we decide to vertically or horizontally flip some panels to ensure X-ray
scattering patterns appear at the bottom right territory.  Morever, cropping the
area that has scattering patterns also enhances model training, as our model
doesn't have to learn from areas with little salient features, e.g. areas of
noise or systematic errors.  Then, bad pixel areas of the pnCCD detector are
masked out with zero values before training.  Since those areas deteriorate over
time, if not masked, they would have steered our model to learn nuanced
differences not intrinsic in scattering patterns, eventually undermining the
model accuracy. Lastly, to improve model accuracy, each panel is normalized by
Z-score normalization, namely allowing the normalized values to have the mean of
zero and the standard deviation of one.  

